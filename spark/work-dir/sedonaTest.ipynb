{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d18fa29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window \n",
    "import pyspark.sql.functions as F\n",
    "from sedona.spark import *\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "845a4f7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: google.cloud.auth.service.account.enable\n",
      "Warning: Ignoring non-Spark config property: google.cloud.auth.service.account.json.keyfile\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/08/24 22:52:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10bae0e292c0:4045\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.6</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>test</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x77d888885f00>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf = SparkConf().set('spark.ui.port', '4045')\\\n",
    "  .set(\"google.cloud.auth.service.account.enable\", \"true\")\\\n",
    "  .set(\"google.cloud.auth.service.account.json.keyfile\", \"/opt/spark/credentials/google-credential.json\")\\\n",
    "  .set(\"spark.serializer\", KryoSerializer.getName)\\\n",
    "  .set(\"spark.kryo.registrator\", SedonaKryoRegistrator.getName)\n",
    "spark = SparkSession.builder.appName(\"test\").config(conf = conf).master(\"local[*]\").getOrCreate()\n",
    "SedonaContext.create(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8a16a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spark_schema():\n",
    "  from pyspark.sql.types import StructType, StructField, IntegerType, TimestampType, FloatType, DoubleType, StringType\n",
    "  return StructType([\n",
    "    StructField(\"MMSI\", StringType(), False),\n",
    "    StructField(\"BaseDateTime\", TimestampType(), False),\n",
    "    StructField(\"LAT\", DoubleType(), False),\n",
    "    StructField(\"LON\", DoubleType(), False),\n",
    "  ])\n",
    "  \n",
    "def get_port_schema():\n",
    "  from pyspark.sql.types import StructType, StructField, IntegerType, TimestampType, FloatType, DoubleType, StringType\n",
    "  return StructType([\n",
    "    StructField(\"UNLOCODE\", StringType(), False),\n",
    "    StructField(\"NAME\", StringType(), False),\n",
    "    StructField(\"STATE\", StringType(), False),\n",
    "    StructField(\"LAT\", DoubleType(), False),\n",
    "    StructField(\"LON\", DoubleType(), False),\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f39e4c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ais_df = spark.read.parquet(\"gs://vessel-traffic-parquet-data/ais_data/year=2024/month=1/part-00165-bf9b1b3a-8f7b-4472-905a-079a799f95ae.c000.snappy.parquet\")\n",
    "port_df = spark.read.csv(\"gs://vessel-traffic-parquet-data/code/ports.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8dc41f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sedona_df = ais_df.select(\"MMSI\", \"BaseDateTime\", ST_Point(F.col(\"LON\"), F.col(\"LAT\")).alias(\"coord\")).alias(\"ais\")\n",
    "port_sedona = port_df.select(\"UNLOCODE\", \n",
    "                             ST_Point(F.col(\"LON\"), F.col(\"LAT\")).alias(\"coord\"), \n",
    "                             ).alias(\"port\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a76b09a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/23 05:56:58 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[MMSI: string, BaseDateTime: timestamp, coord: udt, UNLOCODE: string, coord: udt]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "join_df = sedona_df.join(F.broadcast(port_sedona), ST_Contains(ST_Buffer(F.col(\"port.coord\"), 3500, True),F.col(\"ais.coord\")))\n",
    "join_df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5ba44afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "windowSpec = Window.partitionBy(F.col(\"MMSI\"), F.col(\"BaseDateTime\")).orderBy(F.asc(F.col(\"d\")))\n",
    "join_df = join_df.select(\"MMSI\", \"BaseDateTime\", \"UNLOCODE\", ST_DistanceSpheroid(F.col(\"ais.coord\"), F.col(\"port.coord\")).alias(\"d\"))\\\n",
    "      .select(\"MMSI\", \"BaseDateTime\", \"UNLOCODE\", F.row_number().over(windowSpec).alias(\"r\")).filter(F.col(\"r\") == 1)\\\n",
    "      .select(\"MMSI\", \"BaseDateTime\", \"UNLOCODE\").alias(\"join\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7fca2527",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 58:================================================>         (5 + 1) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+\n",
      "|UNLOCODE|      c|\n",
      "+--------+-------+\n",
      "|    NULL|4844318|\n",
      "|   USSEA|  71897|\n",
      "|   USLGB|  26871|\n",
      "|   USBTR|  24890|\n",
      "|   USPEF|  24518|\n",
      "|   USTPA|  21369|\n",
      "|   USTIW|  20048|\n",
      "|   USLU8|  19040|\n",
      "|   USCRP|  18711|\n",
      "|   USMSY|  15117|\n",
      "|   USHOU|  13077|\n",
      "|   USRCH|   9009|\n",
      "|   USNYC|   7614|\n",
      "|   USHR3|   4858|\n",
      "|   USBPT|   4658|\n",
      "|   USPDX|    638|\n",
      "+--------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "final_df = sedona_df.join(join_df, \n",
    "                           (F.col(\"ais.MMSI\") == F.col(\"join.MMSI\")) & (F.col(\"ais.BaseDateTime\") == F.col(\"join.BaseDateTime\")),\n",
    "                           \"leftouter\"\n",
    "                           )\n",
    "final_df.select(\"ais.MMSI\", \"ais.BaseDateTime\", \"join.UNLOCODE\").groupBy(\"UNLOCODE\").agg(F.count(\"*\").alias(\"c\")).sort(F.desc(\"c\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20f16909",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "test_df = spark.read.parquet(\"gs://vessel-traffic-parquet-data/time_analysis_data/year=2024/month=3/part-00000-a985d02e-8192-4133-938f-2dbaf1e87257.c000.snappy.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5a532e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
